"""
func to count different chunks
"""
import collections
#import spacy
import pandas as pd
import tqdm as tqdm√ü
from nltk.util import ngrams
from nltk.translate.bleu_score import sentence_bleu
#nlp = spacy.load("en_core_web_sm")


def count_NER(text):

    """
    count number of NER
    input: text
    return dataframe of NER count
    """
    doc = nlp(text)
    ent_lst = []
    label_lst = []
    for ent in doc.ents:
        ent_lst.append(ent.text)
        label_lst.append(ent.label_)
    # Group by column 'A' and aggregate column 'B' with count
    df = pd.DataFrame([ent_lst, label_lst]).T
    df.columns = ['entity', 'label']
    ent_df = df.groupby(['entity', 'label']).size().reset_index(name='count')
    ent_df['freq'] = ent_df['count']/len(ent_lst)
    return ent_df



def extract_ngrams(words:list, n:int):
    """Generate n-grams from a list of words"""
    n_grams = list(ngrams(words, n))
    # convert tuple into a string
    output = [' '.join(map(str, t)) for t in n_grams]
    return output

def lowercase_text(text):
    try:
        return text.lower()
    except:
        return text

def count_ngrams(sentences, n:int):
    """count n-grams from a list of words"""
    # preprocess of the utt
    #sentences = col.apply(lowercase_text).tolist() # lower the tokens
    # Convert list of sentences into a single list of words
    word_lst = [word for sentence in sentences for word in str(sentence).split()]
    # extract ngrams
    ngrams = extract_ngrams(word_lst, n)
    # get freq
    frequencyDict = collections.Counter(ngrams)
    freq_lst = list(frequencyDict.values())
    word_lst = list(frequencyDict.keys())
    fre_table = pd.DataFrame([word_lst, freq_lst]).T
    col_Names = ["Word", "Count"]
    fre_table.columns = col_Names
    return fre_table

def self_bleu(reference_sentences, generated_sentences, n):
    total_bleu = 0
    num_generated_sentences = len(generated_sentences)

    # Ensure all elements in reference and generated lists are strings
    reference_sentences = [str(sentence) for sentence in reference_sentences]
    generated_sentences = [str(sentence) for sentence in generated_sentences]

    # Calculate BLEU score for each generated sentence against all reference sentences
    for gen_sentence in generated_sentences:
        bleu_score_sum = 0

        for ref_sentence in reference_sentences:
            # Tokenize sentences
            ref_tokens = ref_sentence.split()
            gen_tokens = gen_sentence.split()

            # Calculate BLEU score
            bleu_score = sentence_bleu([ref_tokens], gen_tokens, weights=(1 / n,) * n)
            bleu_score_sum += bleu_score

        # Average BLEU scores across all reference sentences
        avg_bleu_score = bleu_score_sum / len(reference_sentences)

        # Accumulate total BLEU score
        total_bleu += avg_bleu_score

    # Average BLEU scores across all generated sentences
    avg_bleu = total_bleu / num_generated_sentences

    # Calculate 1 - Self-BLEU score
    self_bleu_score = 1 - avg_bleu

    return self_bleu_score

'''
# Example usage:
reference_sentences = [
    "This is the first reference sentence.",
    "Another reference sentence.",
    "Yet another reference sentence.",
]

generated_sentences = [
    "This is the first reference sentence.",
    "Another sentence generated by the model.",
    "Yet another sentence produced by the model.",
]

self_bleu_score = self_bleu(reference_sentences, generated_sentences)
print("Self-BLEU Score:", self_bleu_score)

# convert the synthesized data into word lists
text_path = '/Users/jliu/PycharmProjects/freq_bias_benchmark/data/generation/unprompted/sample_random/generated/merged/'
out_dir = '/Users/jliu/PycharmProjects/freq_bias_benchmark/data/generation/gen_freq/inv/'
column_list = ['unprompted_0.3','unprompted_0.6','unprompted_1.0','unprompted_1.5']

'''






def get_self_BLEU(text_path:str, column_list:list,n_gram:int):
    """
    get freq from the train/generated tokens
    input:
        path: path to the text path
        column_list: header list for the
    Returns
    -------
    freq table of the gen
    """
    def lowercase_text(text):
        try:
            return text.lower()
        except:
            return text
    bleu_lst = []
    for file in tqdm(os.listdir(text_path)):
        if file.endswith('.csv'):
            text = pd.read_csv(text_path + file)
            reference_sentences = text['train'].apply(lowercase_text).tolist()
            # loop over different temperatures
            for col in column_list:
                # lower the tokens
                generated_sentences = text[col].tolist()
                self_bleu_score = self_bleu(reference_sentences, generated_sentences, n_gram)
                bleu_lst.append(self_bleu_score)
    return bleu_lst


